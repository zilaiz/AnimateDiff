{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import inspect\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from animatediff.models.unet import UNet3DConditionModel\n",
    "from animatediff.models.sparse_controlnet import SparseControlNetModel\n",
    "from animatediff.pipelines.pipeline_animation import AnimationPipeline, AnimationPipelineOutput\n",
    "from animatediff.utils.util import save_videos_grid\n",
    "from animatediff.utils.util import load_weights\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for AnimateDiff\n",
    "pretrained_model_path = \"./models/StableDiffusion\"\n",
    "config_path = \"./configs/prompts/v3/v3-1-T2V.yaml\"\n",
    "inference_config_path = './configs/inference/inference-v3.yaml'\n",
    "time_str = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "savedir = f\"samples/{Path(config_path).stem}-{time_str}\"\n",
    "\n",
    "config = OmegaConf.load(config_path)\n",
    "L, W, H = 16, 512, 512\n",
    "\n",
    "# SD component loading\n",
    "tokenizer    = CLIPTokenizer.from_pretrained(pretrained_model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder=\"text_encoder\").cuda()\n",
    "vae          = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder=\"vae\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 0\n",
    "example_config = config[example_idx]\n",
    "\n",
    "inference_config = OmegaConf.load(example_config['inference_config'])\n",
    "example_config.W = example_config.get(\"W\", W)\n",
    "example_config.L = example_config.get(\"L\", L)\n",
    "example_config.H = example_config.get(\"H\", H)\n",
    "\n",
    "inference_config = OmegaConf.load(example_config.get(\"inference_config\", inference_config_path))\n",
    "\n",
    "# AnimateDiff loading\n",
    "unet = UNet3DConditionModel.from_pretrained_2d(pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs)).cuda()\n",
    "controlnet = None\n",
    "# load controlnet (rgb) and preprocess condition image\n",
    "if example_config.get(\"controlnet_path\", \"\") != \"\":\n",
    "    assert example_config.get(\"controlnet_images\", \"\") != \"\"\n",
    "    assert example_config.get(\"controlnet_config\", \"\") != \"\"\n",
    "    \n",
    "    unet.config.num_attention_heads = 8\n",
    "    unet.config.projection_class_embeddings_input_dim = None\n",
    "\n",
    "    controlnet_config = OmegaConf.load(example_config.controlnet_config)\n",
    "    controlnet = SparseControlNetModel.from_unet(unet, controlnet_additional_kwargs=controlnet_config.get(\"controlnet_additional_kwargs\", {}))\n",
    "\n",
    "    print(f\"loading controlnet checkpoint from {example_config.controlnet_path} ...\")\n",
    "    controlnet_state_dict = torch.load(example_config.controlnet_path, map_location=\"cpu\")\n",
    "    controlnet_state_dict = controlnet_state_dict[\"controlnet\"] if \"controlnet\" in controlnet_state_dict else controlnet_state_dict\n",
    "    controlnet_state_dict.pop(\"animatediff_config\", \"\")\n",
    "    controlnet.load_state_dict(controlnet_state_dict)\n",
    "    controlnet.cuda()\n",
    "\n",
    "    image_paths = example_config.controlnet_images\n",
    "    if isinstance(image_paths, str): image_paths = [image_paths]\n",
    "\n",
    "    print(f\"controlnet image paths:\")\n",
    "    for path in image_paths: print(path)\n",
    "    assert len(image_paths) <= example_config.L\n",
    "\n",
    "    image_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(\n",
    "            (example_config.H, example_config.W), (1.0, 1.0), \n",
    "            ratio=(example_config.W/example_config.H, example_config.W/example_config.H)\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    if example_config.get(\"normalize_condition_images\", False):\n",
    "        def image_norm(image):\n",
    "            image = image.mean(dim=0, keepdim=True).repeat(3,1,1)\n",
    "            image -= image.min()\n",
    "            image /= image.max()\n",
    "            return image\n",
    "    else: image_norm = lambda x: x\n",
    "        \n",
    "    controlnet_images = [image_norm(image_transforms(Image.open(path).convert(\"RGB\"))) for path in image_paths]\n",
    "\n",
    "    os.makedirs(os.path.join(savedir, \"control_images\"), exist_ok=True)\n",
    "    for i, image in enumerate(controlnet_images):\n",
    "        Image.fromarray((255. * (image.numpy().transpose(1,2,0))).astype(np.uint8)).save(f\"{savedir}/control_images/{i}.png\")\n",
    "\n",
    "    controlnet_images = torch.stack(controlnet_images).unsqueeze(0).cuda()\n",
    "    controlnet_images = rearrange(controlnet_images, \"b f c h w -> b c f h w\")\n",
    "\n",
    "    if controlnet.use_simplified_condition_embedding:\n",
    "        num_controlnet_images = controlnet_images.shape[2]\n",
    "        controlnet_images = rearrange(controlnet_images, \"b c f h w -> (b f) c h w\")\n",
    "        controlnet_images = vae.encode(controlnet_images * 2. - 1.).latent_dist.sample() * 0.18215\n",
    "        controlnet_images = rearrange(controlnet_images, \"(b f) c h w -> b c f h w\", f=num_controlnet_images)\n",
    "\n",
    "# set xformers\n",
    "if is_xformers_available():\n",
    "    unet.enable_xformers_memory_efficient_attention()\n",
    "    if controlnet is not None: \n",
    "        controlnet.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "pipeline = AnimationPipeline(\n",
    "    vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
    "    controlnet=controlnet,\n",
    "    scheduler=DDIMScheduler(**OmegaConf.to_container(inference_config.noise_scheduler_kwargs)),\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline = load_weights(\n",
    "    pipeline,\n",
    "    # motion module\n",
    "    motion_module_path         = example_config.get(\"motion_module\", \"\"),\n",
    "    motion_module_lora_configs = example_config.get(\"motion_module_lora_configs\", []),\n",
    "    # domain adapter\n",
    "    adapter_lora_path          = example_config.get(\"adapter_lora_path\", \"\"),\n",
    "    adapter_lora_scale         = example_config.get(\"adapter_lora_scale\", 1.0),\n",
    "    # image layers\n",
    "    dreambooth_model_path      = example_config.get(\"dreambooth_path\", \"\"),\n",
    "    lora_model_path            = example_config.get(\"lora_model_path\", \"\"),\n",
    "    lora_alpha                 = example_config.get(\"lora_alpha\", 0.8),\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "@torch.no_grad()\n",
    "def anmd_sample(\n",
    "    pipeline: AnimationPipeline,\n",
    "    prompt: Union[str, List[str]],\n",
    "    video_length: Optional[int],\n",
    "    height: Optional[int] = None,\n",
    "    width: Optional[int] = None,\n",
    "    num_inference_steps: int = 50,\n",
    "    guidance_scale: float = 7.5,\n",
    "    negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "    num_videos_per_prompt: Optional[int] = 1,\n",
    "    eta: float = 0.0,\n",
    "    generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "    latents: Optional[torch.FloatTensor] = None,\n",
    "    output_type: Optional[str] = \"tensor\",\n",
    "    return_dict: bool = True,\n",
    "    callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "    callback_steps: Optional[int] = 1,\n",
    "\n",
    "    # support controlnet\n",
    "    controlnet_images: torch.FloatTensor = None,\n",
    "    controlnet_image_index: list = [0],\n",
    "    controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n",
    "\n",
    "    **kwargs,\n",
    "):\n",
    "    # Default height and width to unet\n",
    "    height = height or pipeline.unet.config.sample_size * pipeline.vae_scale_factor\n",
    "    width = width or pipeline.unet.config.sample_size * pipeline.vae_scale_factor\n",
    "\n",
    "    # Check inputs. Raise error if not correct\n",
    "    pipeline.check_inputs(prompt, height, width, callback_steps)\n",
    "\n",
    "    # Define call parameters\n",
    "    # batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
    "    batch_size = 1\n",
    "    if latents is not None:\n",
    "        batch_size = latents.shape[0]\n",
    "    if isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "\n",
    "    device = pipeline._execution_device\n",
    "    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "    # corresponds to doing no classifier free guidance.\n",
    "    do_classifier_free_guidance = guidance_scale > 1.0\n",
    "\n",
    "    # Encode input prompt\n",
    "    prompt = prompt if isinstance(prompt, list) else [prompt] * batch_size\n",
    "    if negative_prompt is not None:\n",
    "        negative_prompt = negative_prompt if isinstance(negative_prompt, list) else [negative_prompt] * batch_size \n",
    "    text_embeddings = pipeline._encode_prompt(\n",
    "        prompt, device, num_videos_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    "    )\n",
    "\n",
    "    # Prepare timesteps\n",
    "    pipeline.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = pipeline.scheduler.timesteps\n",
    "\n",
    "    # Prepare latent variables\n",
    "    num_channels_latents = pipeline.unet.in_channels\n",
    "    latents = pipeline.prepare_latents(\n",
    "        batch_size * num_videos_per_prompt,\n",
    "        num_channels_latents,\n",
    "        video_length,\n",
    "        height,\n",
    "        width,\n",
    "        text_embeddings.dtype,\n",
    "        device,\n",
    "        generator,\n",
    "        latents,\n",
    "    )\n",
    "    latents_dtype = latents.dtype\n",
    "\n",
    "    # Prepare extra step kwargs.\n",
    "    extra_step_kwargs = pipeline.prepare_extra_step_kwargs(generator, eta)\n",
    "\n",
    "    # Denoising loop\n",
    "    num_warmup_steps = len(timesteps) - num_inference_steps * pipeline.scheduler.order\n",
    "    with pipeline.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = pipeline.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            down_block_additional_residuals = mid_block_additional_residual = None\n",
    "            if (getattr(pipeline, \"controlnet\", None) != None) and (controlnet_images != None):\n",
    "                assert controlnet_images.dim() == 5\n",
    "\n",
    "                controlnet_noisy_latents = latent_model_input\n",
    "                controlnet_prompt_embeds = text_embeddings\n",
    "\n",
    "                controlnet_images = controlnet_images.to(latents.device)\n",
    "\n",
    "                controlnet_cond_shape    = list(controlnet_images.shape)\n",
    "                controlnet_cond_shape[2] = video_length\n",
    "                controlnet_cond = torch.zeros(controlnet_cond_shape).to(latents.device)\n",
    "\n",
    "                controlnet_conditioning_mask_shape    = list(controlnet_cond.shape)\n",
    "                controlnet_conditioning_mask_shape[1] = 1\n",
    "                controlnet_conditioning_mask          = torch.zeros(controlnet_conditioning_mask_shape).to(latents.device)\n",
    "\n",
    "                assert controlnet_images.shape[2] >= len(controlnet_image_index)\n",
    "                controlnet_cond[:,:,controlnet_image_index] = controlnet_images[:,:,:len(controlnet_image_index)]\n",
    "                controlnet_conditioning_mask[:,:,controlnet_image_index] = 1\n",
    "\n",
    "                down_block_additional_residuals, mid_block_additional_residual = pipeline.controlnet(\n",
    "                    controlnet_noisy_latents, t,\n",
    "                    encoder_hidden_states=controlnet_prompt_embeds,\n",
    "                    controlnet_cond=controlnet_cond,\n",
    "                    conditioning_mask=controlnet_conditioning_mask,\n",
    "                    conditioning_scale=controlnet_conditioning_scale,\n",
    "                    guess_mode=False, return_dict=False,\n",
    "                )\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = pipeline.unet(\n",
    "                latent_model_input, t, \n",
    "                encoder_hidden_states=text_embeddings,\n",
    "                down_block_additional_residuals = down_block_additional_residuals,\n",
    "                mid_block_additional_residual   = mid_block_additional_residual,\n",
    "            ).sample.to(dtype=latents_dtype)\n",
    "\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = pipeline.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "            # call the callback, if provided\n",
    "            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipeline.scheduler.order == 0):\n",
    "                progress_bar.update()\n",
    "                if callback is not None and i % callback_steps == 0:\n",
    "                    callback(i, t, latents)\n",
    "\n",
    "    # Post-processing\n",
    "    video = pipeline.decode_latents(latents)\n",
    "\n",
    "    # Convert to tensor\n",
    "    if output_type == \"tensor\":\n",
    "        video = torch.from_numpy(video)\n",
    "\n",
    "    if not return_dict:\n",
    "        return video\n",
    "\n",
    "    return AnimationPipelineOutput(videos=video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample preparation\n",
    "prompts      = example_config.prompt\n",
    "n_prompts    = list(example_config.n_prompt) * len(prompts) if len(example_config.n_prompt) == 1 else example_config.n_prompt\n",
    "\n",
    "random_seeds = example_config.get(\"seed\", [-1])\n",
    "random_seeds = [random_seeds] if isinstance(random_seeds, int) else list(random_seeds)\n",
    "random_seeds = random_seeds * len(prompts) if len(random_seeds) == 1 else random_seeds\n",
    "\n",
    "example_config.random_seed = []\n",
    "\n",
    "# Sampling\n",
    "samples = []\n",
    "sample_idx = 0\n",
    "for prompt_idx, (prompt, n_prompt, random_seed) in enumerate(zip(prompts, n_prompts, random_seeds)):\n",
    "    print(prompt)\n",
    "    # manually set random seed for reproduction\n",
    "    if random_seed != -1: \n",
    "        torch.manual_seed(random_seed)\n",
    "    else: \n",
    "        torch.seed()\n",
    "    example_config.random_seed.append(torch.initial_seed())\n",
    "\n",
    "    print(f\"current seed: {torch.initial_seed()}\")\n",
    "    print(f\"sampling {prompt} ...\")\n",
    "    sample = anmd_sample(\n",
    "    pipeline,\n",
    "    prompt,\n",
    "    negative_prompt     = n_prompt,\n",
    "    num_inference_steps = example_config.steps,\n",
    "    guidance_scale      = example_config.guidance_scale,\n",
    "    width               = example_config.W,\n",
    "    height              = example_config.H,\n",
    "    video_length        = example_config.L,\n",
    "\n",
    "    controlnet_images = controlnet_images,\n",
    "    controlnet_image_index = example_config.get(\"controlnet_image_indexs\", [0]),\n",
    "    ).videos\n",
    "\n",
    "    samples.append(sample)\n",
    "\n",
    "    prompt = \"-\".join((prompt.replace(\"/\", \"\").split(\" \")[:10]))\n",
    "    save_videos_grid(sample, f\"{savedir}/sample/{sample_idx}-{prompt}.gif\")\n",
    "    print(f\"save to {savedir}/sample/{prompt}.gif\")\n",
    "\n",
    "    sample_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
